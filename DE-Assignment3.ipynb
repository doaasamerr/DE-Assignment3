{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = spark.read.format(\"csv\")\\\n",
    " .option(\"header\", \"true\")\\\n",
    " .option(\"inferSchema\", \"true\")\\\n",
    " .load(\"/Users/doaasamer/Downloads/Spark-The-Definitive-Guide/data/retail-data/by-day/*.csv\")\\\n",
    " .coalesce(5)\\\n",
    " .where(\"Description IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load a dataframe we use the .read function. We can also specify the format that will be used by the .format function where we should write their fully qualified name (i.e., org.apache.spark.sql.parquet) but since there are built-ins we can use their short names (parquet, json, csv, text..). The .option function specifies extra options when loading the dataframe(also used during write operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleDF = spark.read.json(\"/Users/doaasamer/Downloads/Spark-The-Definitive-Guide/data/simple-ml\")\n",
    "scaleDF = spark.read.parquet(\"/Users/doaasamer/Downloads/Spark-The-Definitive-Guide/data/simple-ml-scaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read.json automatically infers the schema of a json dataset and load it as a Dataset[Row]. \n",
    "And read.parquet reads from parquet format. The result of loading a parquet file is also a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------------------------+\n",
      "| id|      features|StandardScaler_210e0a72169b__output|\n",
      "+---+--------------+-----------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
      "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
      "|  1|[3.0,10.1,3.0]|               [3.58568582800318...|\n",
      "+---+--------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "sScaler = StandardScaler().setInputCol(\"features\")\n",
    "sScaler.fit(scaleDF).transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The StandardScalar standardizes each feature by removing the mean(=0) and scaling to unit variance(=1) using column summary statistics on the samples in the training set before applying machine learning techniques. It normalizes each feature(column) individually. In our example, we apply the StandardScalar to scaleDF where the output column has the new normalized values with unit variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------+\n",
      "| id|      features|MinMaxScaler_9a3bd61b8321__output|\n",
      "+---+--------------+---------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                    [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|                    [7.5,5.5,7.5]|\n",
      "|  0|[1.0,0.1,-1.0]|                    [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|                    [7.5,5.5,7.5]|\n",
      "|  1|[3.0,10.1,3.0]|                 [10.0,10.0,10.0]|\n",
      "+---+--------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "minMax = MinMaxScaler().setMin(5).setMax(10).setInputCol(\"features\")\n",
    "fittedminMax = minMax.fit(scaleDF)\n",
    "fittedminMax.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MinMaxScaler transforms a dataset of Vector rows, rescaling each feature individually to a specific range (often [0, 1]) linearly using column summary statistics. It takes(min, max) as parameters. The rescaled value is caluclated using the formula Rescaled(ei)=ei−EminEmax−Emin∗(max−min)+min. In this exmaple, the min is set to 5 and the max is set to 10 where the new rescaled results are represented in the output column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------------------------------+\n",
      "| id|      features|Normalizer_f8cbac5a0758__output|\n",
      "+---+--------------+-------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|           [0.47619047619047...|\n",
      "|  1| [2.0,1.1,1.0]|           [0.48780487804878...|\n",
      "|  0|[1.0,0.1,-1.0]|           [0.47619047619047...|\n",
      "|  1| [2.0,1.1,1.0]|           [0.48780487804878...|\n",
      "|  1|[3.0,10.1,3.0]|           [0.18633540372670...|\n",
      "+---+--------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "manhattanDistance = Normalizer().setP(1).setInputCol(\"features\")\n",
    "manhattanDistance.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Normalizer transforms a dataset of Vector rows, normalizing each Vector to have unit norm. It takes parameter p, which specifies the p-norm used for normalization. (p=2 by default.) This normalization can help standardize input data and improve the behavior of learning algorithms. However, the p is set to 1 here in this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+\n",
      "|color| lab|value1|            value2|labelInd|\n",
      "+-----+----+------+------------------+--------+\n",
      "|green|good|     1|14.386294994851129|     1.0|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     1.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "|green| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    35|14.386294994851129|     1.0|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|\n",
      "|  red| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    45| 38.97187133755819|     1.0|\n",
      "|green|good|     1|14.386294994851129|     1.0|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     1.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "|green| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    35|14.386294994851129|     1.0|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|\n",
      "+-----+----+------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "lblIndxr = StringIndexer().setInputCol(\"lab\").setOutputCol(\"labelInd\")\n",
    "idxRes = lblIndxr.fit(simpleDF).transform(simpleDF)\n",
    "idxRes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The StringIndexer encodes a string column of labels to a column of label indices.  If the input column is numeric, we cast it to string and index the string values. It has 4 supported ordering options(frequencyDesc, alphabetDesc, frequencyAsc, alphabetAsc) where the most frequent label is assigned 0. In our case, we apply it now on the simpleDF table and the output indicates that \"bad\" is the most frequent label hence given the value 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+\n",
      "|color| lab|value1|            value2|valueInd|\n",
      "+-----+----+------+------------------+--------+\n",
      "|green|good|     1|14.386294994851129|     2.0|\n",
      "| blue| bad|     8|14.386294994851129|     4.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     5.0|\n",
      "|green|good|    12|14.386294994851129|     0.0|\n",
      "|green| bad|    16|14.386294994851129|     1.0|\n",
      "|  red|good|    35|14.386294994851129|     6.0|\n",
      "|  red| bad|     1| 38.97187133755819|     2.0|\n",
      "|  red| bad|     2|14.386294994851129|     7.0|\n",
      "|  red| bad|    16|14.386294994851129|     1.0|\n",
      "|  red|good|    45| 38.97187133755819|     3.0|\n",
      "|green|good|     1|14.386294994851129|     2.0|\n",
      "| blue| bad|     8|14.386294994851129|     4.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     5.0|\n",
      "|green|good|    12|14.386294994851129|     0.0|\n",
      "|green| bad|    16|14.386294994851129|     1.0|\n",
      "|  red|good|    35|14.386294994851129|     6.0|\n",
      "|  red| bad|     1| 38.97187133755819|     2.0|\n",
      "|  red| bad|     2|14.386294994851129|     7.0|\n",
      "+-----+----+------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valIndexer = StringIndexer().setInputCol(\"value1\").setOutputCol(\"valueInd\")\n",
    "valIndexer.fit(simpleDF).transform(simpleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas in the previous part we applied the StringIndexer on the column \"lab\" showing that \"bad\" is the most frequent label, in this part, we apply it on \"value1\" column showing that the most frequent label in this case is \"12\" as its given the value 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----------------------------------+\n",
      "|color|colorInd|OneHotEncoder_278c4e7ab963__output|\n",
      "+-----+--------+----------------------------------+\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "| blue|     2.0|                         (2,[],[])|\n",
      "| blue|     2.0|                         (2,[],[])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "| blue|     2.0|                         (2,[],[])|\n",
      "| blue|     2.0|                         (2,[],[])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "+-----+--------+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "lblIndxr = StringIndexer().setInputCol(\"color\").setOutputCol(\"colorInd\")\n",
    "colorLab = lblIndxr.fit(simpleDF).transform(simpleDF.select(\"color\"))\n",
    "ohe = OneHotEncoder().setInputCol(\"colorInd\")\n",
    "ohe.transform(colorLab).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OneHotEncoder maps a categorical feature, represented as a label index, to a binary vector with at most a single one-value indicating the presence of a specific feature value from among the set of all feature values. This encoding allows algorithms which expect continuous features to use categorical features. It is common for string type inputs to encode using StringIndexer first which was done here in this example where we applied it on the column \"color\" in the table \"simpleDF\" resulting in the column \"colorInd\" then the OneHotEncoder was applied resulting in the output column. Since the first part \"colorInd\" resulted in 3 values {0.0, 1.0, 2.0}, the vector in the output column is of length 2 where (length, position of 1.0, 1.0) hence for 2.0 the last 2 entries in the vector are empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+\n",
      "|Description                        |DescOut                                   |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "|RABBIT NIGHT LIGHT                 |[rabbit, night, light]                    |\n",
      "|DOUGHNUT LIP GLOSS                 |[doughnut, lip, gloss]                    |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES    |[12, message, cards, with, envelopes]     |\n",
      "|BLUE HARMONICA IN BOX              |[blue, harmonica, in, box]                |\n",
      "|GUMBALL COAT RACK                  |[gumball, coat, rack]                     |\n",
      "|SKULLS  WATER TRANSFER TATTOOS     |[skulls, water, transfer, tattoos]        |\n",
      "|FELTCRAFT GIRL AMELIE KIT          |[feltcraft, girl, amelie, kit]            |\n",
      "|CAMOUFLAGE LED TORCH               |[camouflage, led, torch]                  |\n",
      "|WHITE SKULL HOT WATER BOTTLE       |[white, skull, hot, water, bottle]        |\n",
      "|ENGLISH ROSE HOT WATER BOTTLE      |[english, rose, hot, water, bottle]       |\n",
      "|HOT WATER BOTTLE KEEP CALM         |[hot, water, bottle, keep, calm]          |\n",
      "|SCOTTIE DOG HOT WATER BOTTLE       |[scottie, dog, hot, water, bottle]        |\n",
      "|ROSE CARAVAN DOORSTOP              |[rose, caravan, doorstop]                 |\n",
      "|GINGHAM HEART  DOORSTOP RED        |[gingham, heart, doorstop, red]           |\n",
      "|STORAGE TIN VINTAGE LEAF           |[storage, tin, vintage, leaf]             |\n",
      "|SET OF 4 KNICK KNACK TINS POPPIES  |[set, of, 4, knick, knack, tins, poppies] |\n",
      "|POPCORN HOLDER                     |[popcorn, holder]                         |\n",
      "|GROW A FLYTRAP OR SUNFLOWER IN TIN |[grow, a, flytrap, or, sunflower, in, tin]|\n",
      "|AIRLINE BAG VINTAGE WORLD CHAMPION |[airline, bag, vintage, world, champion]  |\n",
      "|AIRLINE BAG VINTAGE JET SET BROWN  |[airline, bag, vintage, jet, set, brown]  |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "rt = RegexTokenizer()\\\n",
    " .setInputCol(\"Description\")\\\n",
    " .setOutputCol(\"DescOut\")\\\n",
    " .setPattern(\" \")\\\n",
    " .setToLowercase(True)\n",
    "rt.transform(sales.select(\"Description\")).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of taking text (such as a sentence) and breaking it into individual terms (usually words). A simple Tokenizer class provides this functionality. RegexTokenizer allows more advanced tokenization based on regular expression (regex) matching. In our example, the pattern given to the Tokenizer is the \" \", hence the output contains all the space seperated words in the column \"Description\" for each entry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fittedCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-31dd4d668601>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m  \u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msales\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Description\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CustomerId\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m  \u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CustomerId IS NOT NULL\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprechi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfittedCV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m  \u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CustomerId IS NOT NULL\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mchisq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChiSqSelector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fittedCV' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector, Tokenizer\n",
    "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "tokenized = tkn\\\n",
    " .transform(sales.select(\"Description\", \"CustomerId\"))\\\n",
    " .where(\"CustomerId IS NOT NULL\")\n",
    "prechi = fittedCV.transform(tokenized)\\\n",
    " .where(\"CustomerId IS NOT NULL\")\n",
    "chisq = ChiSqSelector()\\\n",
    " .setFeaturesCol(\"countVec\")\\\n",
    " .setLabelCol(\"CustomerId\")\\\n",
    " .setNumTopFeatures(2)\n",
    "chisq.fit(prechi).transform(prechi)\\\n",
    " .drop(\"customerId\", \"Description\", \"DescOut\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(depends on fitting the model to the dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
